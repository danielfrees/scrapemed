{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#!conda list\n",
    "#just checking that vscode was correctly syncing with conda env\n",
    "\n",
    "#sample article to work with\n",
    "#\"Ibuprofen for acute treatment of episodic tension‐type headache in adults\"\n",
    "#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6457940/\n",
    "\n",
    "#IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "#from requests_html import HTMLSession\n",
    "import requests\n",
    "from requests.exceptions import ConnectionError"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Paper Scraping Exploration \n",
    "\n",
    "*Author: Daniel Frees, Email: daniuelfrees247@gmail.com, Last Updated: 02/22/23*\n",
    "\n",
    "*(Goal: Parse PMC Articles into Standard Format, Evaluate Parse Quality, Store in DB)*\n",
    "\n",
    "1. Search for relevant papers via NCBI\n",
    "    >todo: focusing on a small sample of papers first  \n",
    "    >notes: will use Entrez  \n",
    "\n",
    "1. Access XML via NCBI √\n",
    "    >notes: using Biopython's Entrez to access the NCBI's XML data for papers. Yields useful metadata\n",
    "\n",
    "1. Access full paper data via NCBI √\n",
    "    >Tried So Far:\n",
    "    >>•Entrez √ \n",
    "      \n",
    "    >>•BioC RESTful API (Pubmed says this yields full data for Open Access articles, but it does not in any of my experimentation here, regardless of json vs. xml).  \n",
    "      \n",
    "    >>•FTP should supposedly work (https://www.biostars.org/p/159761/), but seems to be a convoluted mess. Sticking with Entrez.\n",
    "\n",
    "1. Convert downloaded full paper data into standard format [IN PROGRESS]\n",
    "    >notes: use ElementTree library and XPath to parse XMLs  \n",
    "    >[XML NCBI Guidelines](https://www.ncbi.nlm.nih.gov/pmc/pmcdoc/tagging-guidelines/article/style.html)  \n",
    "    >can run with retmode = 'json' but does not return most of the needed information. Unfortunate as this could have been useful for identifying problematic html styling tags \n",
    "    >Use json request from Entrez to identify html tags that need removal, remove them and use Xpath on the resulting XML to create standardized format.\n",
    "\n",
    "1. Evaluate success of conversion (0 = perfect, 1 = success, but not all sections found, 2 = failure)\n",
    "\n",
    "1. Use SQL Alchemy to store perfect papers in a table, successful imperfect papers in another table"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "#SET UP desired format for papers retrieved\n",
    "#Using an object so this can be SQL Alchemy friendly when I scale to connection with the backend\n",
    "from sqlalchemy import create_engine, ForeignKey, Column, String, Integer, CHAR, Boolean\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class Paper(Base):\n",
    "    __tablename__ = \"Papers\"\n",
    "\n",
    "    #paper identifiers, DOI should be a completely unique string and is PK\n",
    "    doi = Column(\"doi\", String, primary_key=True)\n",
    "    title = Column(\"title\", String)\n",
    "    pmid = Column(\"pmid\", String)  #for articles in PMC\n",
    "    authors = Column(\"authors\", String)  #list of authors of paper, could potentially blow out into multiple columns later\n",
    "    journal = Column(\"journal\", String)\n",
    "\n",
    "    #included in case some papers don't properly parse into the paper sections following\n",
    "    #papers are very very frequently formatted different ways and may contain extra sections/\n",
    "    #be missing sections from the following. \n",
    "    #so full_text will likely need to be a fall back for most papers\n",
    "    full_text = Column(\"full_text\", String) \n",
    "\n",
    "    #paper sections\n",
    "    abstract = Column(\"abstract\", String)\n",
    "    background = Column(\"background\", String)\n",
    "    methods = Column(\"methods\", String)\n",
    "    results = Column(\"results\", String)\n",
    "    discussion = Column(\"discussion\", String)\n",
    "    conclusion = Column(\"conclusion\", String)\n",
    "    acknowledgements = Column(\"acknowledgements\", String)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BioC API Requests"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import requests\n",
    "from requests.exceptions import ConnectionError\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "#Basic REST API endpoint provided by NCBI. Doesn't seem to retrieve the entire paper\n",
    "#set up paper request\n",
    "def get_bioc_data(pmid, file_format = 'json', encoding = 'unicode', verbose = False):\n",
    "    PMID =  pmid    #for ibuprofen: \"26230487\", for more standard format \"\n",
    "    req = f\"https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pubmed.cgi/BioC_{file_format}/{PMID}/{encoding}\"\n",
    "\n",
    "    response = requests.get(req)\n",
    "   \n",
    "    if verbose:\n",
    "        print(f\"Response type received: {type(response)}\")\n",
    "\n",
    "    if file_format == 'json':\n",
    "        data = response.json()\n",
    "    elif file_format == 'xml':\n",
    "        data = ET.ElementTree(ET.fromstring(response.text))\n",
    "    else:\n",
    "        print(\"No data to return. Please specify a file_format of json or xml.\")\n",
    "    return data\n",
    "\n",
    "#Ibuprofen article\n",
    "#get_bioc_data(26230487)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#Cetirizine in the dog\n",
    "#get_bioc_data(30477556)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#Open Access children ibuprofen use lit review\n",
    "#get_bioc_data(28597358)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "#definitely open access\n",
    "#get_bioc_data(36629266)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#definitely open access\n",
    "oa_tree = get_bioc_data(36629266, file_format = 'xml', verbose = True)\n",
    "element = oa_tree.getroot()\n",
    "ET.indent(element)\n",
    "print(ET.tostring(element, encoding='unicode'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Response type received: <class 'requests.models.Response'>\n",
      "<collection>\n",
      "  <source>PubMed</source>\n",
      "  <date>20230225</date>\n",
      "  <key>collection.key</key>\n",
      "  <document>\n",
      "    <id>36629266</id>\n",
      "    <passage>\n",
      "      <infon key=\"type\">title</infon>\n",
      "      <offset>0</offset>\n",
      "      <text>Hypothesis-driven probabilistic modelling enables a principled perspective of genomic compartments.</text>\n",
      "    </passage>\n",
      "    <passage>\n",
      "      <infon key=\"type\">abstract</infon>\n",
      "      <offset>100</offset>\n",
      "      <text>The Hi-C method has revolutionized the study of genome organization, yet interpretation of Hi-C interaction frequency maps remains a major challenge. Genomic compartments are a checkered Hi-C interaction pattern suggested to represent the partitioning of the genome into two self-interacting states associated with active and inactive chromatin. Based on a few elementary mechanistic assumptions, we derive a generative probabilistic model of genomic compartments, called deGeco. Testing our model, we find it can explain observed Hi-C interaction maps in a highly robust manner, allowing accurate inference of interaction probability maps from extremely sparse data without any training of parameters. Taking advantage of the interpretability of the model parameters, we then test hypotheses regarding the nature of genomic compartments. We find clear evidence of multiple states, and that these states self-interact with different affinities. We also find that the interaction rules of chromatin states differ considerably within and between chromosomes. Inspecting the molecular underpinnings of a four-state model, we show that a simple classifier can use histone marks to predict the underlying states with 87% accuracy. Finally, we observe instances of mixed-state loci and analyze these loci in single-cell Hi-C maps, finding that mixing of states occurs mainly at the cell level.</text>\n",
      "    </passage>\n",
      "  </document>\n",
      "</collection>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Entrez Data Collection, HTML tag removal, XML Parsing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import re\n",
    "\n",
    "#Cleanup record bytes HTML styling\n",
    "def remove_html_styling(text, removals = [\"<italic>\", \"<i>\", \"<bold\", \"<b>\", \"<underline>\", \"<u>\"], replaces = {\"<sub>\": \"_\"}, verbose = False):\n",
    "    \"\"\"\n",
    "    Removes <italic>,<i>,<bold>,<b>,<underline>,<u> opening and closing tags.\n",
    "    Replaces <sub> opening tags with _, and removes <sub> closing tags.\n",
    "\n",
    "    html tags should not contain regex special characters, hence no escaping necessary. \n",
    "    \"\"\"\n",
    "    to_remove = removals\n",
    "    more_to_remove = []\n",
    "    to_replace = replaces \n",
    "    for tag in to_remove:\n",
    "        more_to_remove.append(tag[0] + \"/\" + tag[1:]) #remove closing tags as well\n",
    "    to_remove.extend(more_to_remove)\n",
    "    for tag in to_replace.keys():\n",
    "        to_remove.append(tag[0] + \"/\" + tag[1:]) #remove closing tags for those opening tags being replaced\n",
    "    if verbose:\n",
    "        print(\"Removing the following tags:\\n\")\n",
    "        print(to_remove)\n",
    "        print()\n",
    "        print(\"Making the following replacements:\\n\")\n",
    "        for find, replace in to_replace.items():\n",
    "            print(f\"{find} replaced with {replace}\\n\")\n",
    "    removal_pattern = '|'.join(to_remove)\n",
    "    text = re.sub(removal_pattern, '', text)\n",
    "    for find, replace in to_replace.items():\n",
    "        text = re.sub(find, replace, text)\n",
    "    return text\n",
    "\n",
    "test_text = (\"Hello my name is Daniel, my <italic>favorite</italic> chemical is <i>C</i><sub>4</sub>. \"\n",
    "\"<b>I</b> also wanted to say that <underline>you</underline> should <u>use this code as a test to make sure \"\n",
    "\"html tagging removal is going as expected.\")\n",
    "remove_html_styling(test_text, verbose = True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Removing the following tags:\n",
      "\n",
      "['<italic>', '<i>', '<bold', '<b>', '<underline>', '<u>', '</italic>', '</i>', '</bold', '</b>', '</underline>', '</u>', '</sub>']\n",
      "\n",
      "Making the following replacements:\n",
      "\n",
      "<sub> replaced with _\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Hello my name is Daniel, my favorite chemical is C_4. I also wanted to say that you should use this code as a test to make sure html tagging removal is going as expected.'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#Use Entrez to request data. This library was built specifically for querying NCBI databases.\n",
    "from Bio import Entrez\n",
    "\n",
    "#Credential and PMCID\n",
    "Entrez.email = 'danielfrees247@gmail.com'\n",
    "PMCID = 7067710 #Acetaminophen and Ibuprofen Study\n",
    "\n",
    "#GET XML\n",
    "handle = Entrez.efetch(db = 'pmc', id = PMCID, rettype = 'full', retmode = 'xml')\n",
    "xml_record = handle.read()\n",
    "print(f\"XML Record First 100 bytes: {xml_record[0:100]}\")\n",
    "\n",
    "#GET JSON (runs but does not yield most of the information unfortunately)\n",
    "#handle = Entrez.efetch(db = 'pmc', id = PMCID, rettype = 'full', retmode = 'json')\n",
    "#json_record = handle.read()\n",
    "#print(f\"JSON Record First 100 bytes: {json_record[0:100]}\")\n",
    "\n",
    "#Save text of the XML for investigation\n",
    "with open(f\"data/entrez_download_PMCID={PMCID}_xml.txt\", \"w\") as f:\n",
    "    f.write(xml_record.decode(encoding = \"utf-8\"))\n",
    "#Save text of the JSON for investigation\n",
    "#with open(f\"data/entrez_download_PMCID={PMCID}_json.txt\", \"w\") as f:\n",
    "    #f.write(json_record.decode(encoding = \"utf-8\"))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "XML Record First 100 bytes: b'<?xml version=\"1.0\" ?>\\n<!DOCTYPE pmc-articleset PUBLIC \"-//NLM//DTD ARTICLE SET 2.0//EN\" \"https://dt'\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\n",
    "#Visualize XML tree with graph viz\n",
    "from graphviz import Digraph\n",
    "\n",
    "def visualize_element_tree(element, title = 'data/element_tree.gv'):\n",
    "    \"\"\"Visualize an XML element tree using Graphviz.\"\"\"\n",
    "    dot = Digraph()\n",
    "    _add_elements(dot, element)\n",
    "    dot.render(title, view=True)\n",
    "\n",
    "def _add_elements(dot, element, parent=None):\n",
    "    \"\"\"Recursively add elements to a Graphviz dot graph.\"\"\"\n",
    "    if parent is not None:\n",
    "        dot.edge(parent, element.tag)\n",
    "    dot.node(element.tag, element.tag)\n",
    "    for child in element:\n",
    "        _add_elements(dot, child, element.tag)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize XML Tree"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "#Convert bytestream to XML tree\n",
    "record_tree = ET.ElementTree(ET.fromstring(xml_record))\n",
    "\n",
    "#Parse XML Tree\n",
    "root = record_tree.getroot()\n",
    "print(f\"\\nXML Tree Info (PMCID:{PMCID})\")\n",
    "print(\"------------------------------\")\n",
    "print(f\"Root Tag: {root.tag}\")\n",
    "print(f\"Root Attributes: {root.attrib}\\n\")\n",
    "print(\"------------------------------\")\n",
    "for child in root:\n",
    "    print(f\"\\tChild Tag: {child.tag}\")\n",
    "    print(f\"\\tChild Attributes: {child.attrib}\\n\")\n",
    "\n",
    "all_element_types = set([elem.tag for elem in root.iter()])\n",
    "print(f\"Unique Element Types: {all_element_types}\")\n",
    "\n",
    "#Visualize uncleaned tree\n",
    "visualize_element_tree(root, title = f\"data/{PMCID}_element_tree.gv\")\n",
    "#These are a whole mess for PMID articles it seems as the tree is quite large\n",
    "#Furthermore, there are some problematic html stylings with italics etc. which can cause data loss\n",
    "\n",
    "#Visualize tree after removing problematic html styling\n",
    "xml_record_text = xml_record.decode(encoding = \"utf-8\")\n",
    "xml_record_clean = remove_html_styling(xml_record_text)\n",
    "clean_tree = ET.ElementTree(ET.fromstring(xml_record_clean))\n",
    "clean_root = clean_tree.getroot()\n",
    "visualize_element_tree(clean_root, title = f\"data/{PMCID}_element_tree_clean.gv\")\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "XML Tree Info (PMCID:7067710)\n",
      "------------------------------\n",
      "Root Tag: pmc-articleset\n",
      "Root Attributes: {}\n",
      "\n",
      "------------------------------\n",
      "\tChild Tag: article\n",
      "\tChild Attributes: {'article-type': 'research-article'}\n",
      "\n",
      "Unique Element Types: {'td', 'person-group', 'lpage', 'copyright-statement', 'article', 'title-group', 'given-names', 'source', 'fn-group', 'award-group', 'license', 'article-meta', 'tbody', 'abstract', 'p', 'sub', 'issn', 'institution-id', 'journal-id', 'suffix', 'table-wrap-foot', 'ref-list', 'subject', 'tr', 'notes', 'funding-source', 'label', 'month', 'year', 'ext-link', 'day', 'element-citation', 'journal-title-group', 'sec', 'address', 'meta-value', 'fig', 'break', 'volume', 'mixed-citation', 'permissions', 'body', 'table', 'pub-date', 'institution', 'issue', 'thead', 'etal', 'email', 'th', 'article-id', 'journal-title', 'custom-meta', 'ref', 'italic', 'publisher-loc', 'bold', 'contrib-group', 'front', 'xref', 'ack', 'custom-meta-group', 'pmc-articleset', 'meta-name', 'article-title', 'back', 'surname', 'institution-wrap', 'fpage', 'article-categories', 'publisher', 'pub-id', 'aff', 'sup', 'name', 'license-p', 'sc', 'title', 'subj-group', 'publisher-name', 'graphic', 'caption', 'contrib', 'table-wrap', 'fn', 'journal-meta', 'funding-group'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#XPath Parsing functions\n",
    "\n",
    "#Work with the cleaned data\n",
    "root = clean_root\n",
    "\n",
    "#Parsing abstract\n",
    "root_path = './/*/abstract/'\n",
    "num_abstract_sections = len(root.findall(root_path)) #grab number of abstract sections\n",
    "for i in range (1, num_abstract_sections+1):\n",
    "    section_path = root_path + f\"/sec[{i}]/\"\n",
    "    section_title_path = section_path + \"title\"\n",
    "    section_text_path = section_path + \"p\"\n",
    "    print(f\"Section: {root.findall(section_title_path)[0].text}--------------\")\n",
    "    print(root.findall(section_text_path)[0].text)\n",
    "    print()\n",
    "    print(\"-----------------------------------------\\n\")\n",
    "\n",
    "#Parsing body\n",
    "num_body_sections = len(root.findall('.//*body/')) #grab number of sections"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Section: Introduction--------------\n",
      "A fixed-dose combination (FDC) of ibuprofen and acetaminophen has been developed that provides greater analgesic efficacy than either agent alone at the same doses without increasing the risk for adverse events.\n",
      "\n",
      "-----------------------------------------\n",
      "\n",
      "Section: Methods--------------\n",
      "We report three clinical phase I studies designed to assess the pharmacokinetics (PK) of the FDC of ibuprofen/acetaminophen 250/500 mg (administered as two tablets of ibuprofen 125 mg/acetaminophen 250 mg) in comparison with its individual components administered alone or together, and to determine the effect of food on the PK of the FDC. Two studies in healthy adults aged 18–55 years used a crossover design in which subjects received a single dose of each treatment with a 2-day washout period between each. In the third study, the bioavailability of ibuprofen and acetaminophen from a single oral dose of the FDC was assessed in healthy adolescents aged 12–17 years, inclusive.\n",
      "\n",
      "-----------------------------------------\n",
      "\n",
      "Section: Results--------------\n",
      "A total of 35 and 46 subjects were enrolled in the two adult studies, respectively, and 21 were enrolled in the adolescent study. Ibuprofen and acetaminophen in the FDC were bioequivalent to the monocomponents administered alone or together. With food, the maximum concentration (C_max) for ibuprofen and acetaminophen from the FDC was reduced by 36% and 37%, respectively, and time to C_max (i.e. t_max) was delayed. Overall drug exposure to ibuprofen or acetaminophen in the fed versus fasted states was similar. In adolescents, overall exposure to acetaminophen and ibuprofen was comparable with that in adults, with a slightly higher overall exposure to ibuprofen. Exposure to acetaminophen and ibuprofen in adolescents aged 12–14 years was slightly higher versus those aged 15–17 years. Adverse events were similar across all treatment groups.\n",
      "\n",
      "-----------------------------------------\n",
      "\n",
      "Section: Conclusions--------------\n",
      "The FDC of ibuprofen/acetaminophen 250/500 mg has a PK profile similar to its monocomponent constituents when administered separately or coadministered, indicating no drug–drug interactions and no formulation effects. Similar to previous findings for the individual components, the rates of absorption of ibuprofen and acetaminophen from the FDC were slightly delayed in the presence of food. Overall, adolescents had similar exposures to acetaminophen and ibuprofen as adults, while younger adolescents had slightly greater exposure than older adolescents, probably due to their smaller body size. The FDC was generally well tolerated.\n",
      "\n",
      "-----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#root.findall(\".//*/abstract/sec[3]/p/italic[1]\")[0].text\n",
    "#no longer a problem since we've cleaned the tree of styling"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "root.findall(\".//*/table/\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<Element 'tbody' at 0x7fb0869da250>,\n",
       " <Element 'thead' at 0x7fb0869e5080>,\n",
       " <Element 'tbody' at 0x7fb0869e5440>,\n",
       " <Element 'thead' at 0x7fb0869e78d0>,\n",
       " <Element 'tbody' at 0x7fb0869e7e70>,\n",
       " <Element 'thead' at 0x7fb0869f87c0>,\n",
       " <Element 'tbody' at 0x7fb0869f8d60>,\n",
       " <Element 'thead' at 0x7fb0869fa750>,\n",
       " <Element 'tbody' at 0x7fb0869faca0>,\n",
       " <Element 'thead' at 0x7fb086a088b0>,\n",
       " <Element 'tbody' at 0x7fb086a08db0>,\n",
       " <Element 'thead' at 0x7fb086a0aa70>,\n",
       " <Element 'tbody' at 0x7fb086a0b060>,\n",
       " <Element 'thead' at 0x7fb086a20bd0>,\n",
       " <Element 'tbody' at 0x7fb086a21030>,\n",
       " <Element 'thead' at 0x7fb086a23e70>,\n",
       " <Element 'tbody' at 0x7fb086a30540>,\n",
       " <Element 'thead' at 0x7fb086a32890>,\n",
       " <Element 'tbody' at 0x7fb086a33510>]"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#Current roadblock is that I think I may need to fork and PR Biopython to fix their math-formatted text parsing. Text breaks when reading in any italics, the italics are saved in a separate path but the rest of the non-italicized text disappears."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.10.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.9 64-bit ('scrapemed': conda)"
  },
  "interpreter": {
   "hash": "427d4943968a1e3fc165f884a0cbd733d19eb316925f7601ef95e6c86f630a99"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}